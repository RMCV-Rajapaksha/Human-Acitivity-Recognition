 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UCF50 Action Recognition with ConvLSTM\n",
    "\n",
    "This notebook implements an action recognition system using the UCF50 dataset with enhanced preprocessing techniques and a ConvLSTM-based neural network architecture. The implementations include:\n",
    "\n",
    "1. Automatic class detection from UCF50 directory\n",
    "2. Multiple preprocessing options:\n",
    "   - Optical flow computation\n",
    "   - Background subtraction\n",
    "   - Data augmentation\n",
    "   - Color space conversion\n",
    "3. Two model architectures:\n",
    "   - Enhanced single-stream ConvLSTM\n",
    "   - Two-stream network (RGB + optical flow)\n",
    "4. Detailed visualization and evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "import tensorflow as tf\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from scipy import ndimage\n",
    "import albumentations as A\n",
    "\n",
    "# Check versions\n",
    "print(f\"TensorFlow Version: {tf.__version__}\")\n",
    "print(f\"OpenCV Version: {cv2.__version__}\")\n",
    "print(f\"NumPy Version: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seeds for reproducibility\n",
    "SEED_CONSTANT = 27\n",
    "np.random.seed(SEED_CONSTANT)\n",
    "random.seed(SEED_CONSTANT)\n",
    "tf.random.set_seed(SEED_CONSTANT)\n",
    "\n",
    "# Basic parameters\n",
    "DATASET_PATH = \"C:/Users/ROG/OneDrive/Desktop/Projects/Sem 6 AI/UCF50\"\n",
    "IMAGE_HEIGHT, IMAGE_WIDTH = 64, 64\n",
    "SEQUENCE_LENGTH = 20\n",
    "\n",
    "# Automatically detect all classes from the dataset directory\n",
    "CLASSES_LIST = [folder_name for folder_name in os.listdir(DATASET_PATH) \n",
    "                if os.path.isdir(os.path.join(DATASET_PATH, folder_name))]\n",
    "print(f\"Found {len(CLASSES_LIST)} classes in the dataset:\")\n",
    "print(CLASSES_LIST)\n",
    "\n",
    "# If you want to limit the number of classes for faster training, uncomment and modify:\n",
    "# CLASSES_LIST = CLASSES_LIST[:5]  # Using only the first 5 classes\n",
    "# print(f\"Using {len(CLASSES_LIST)} classes for training:\")\n",
    "# print(CLASSES_LIST)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Preprocessing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_optical_flow(previous_frame, current_frame):\n",
    "    \"\"\"Compute optical flow between two consecutive frames.\"\"\"\n",
    "    # Convert frames to grayscale\n",
    "    prev_gray = cv2.cvtColor(previous_frame, cv2.COLOR_BGR2GRAY)\n",
    "    curr_gray = cv2.cvtColor(current_frame, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Calculate optical flow using Farneback method\n",
    "    flow = cv2.calcOpticalFlowFarneback(\n",
    "        prev_gray, curr_gray, None, 0.5, 3, 15, 3, 5, 1.2, 0)\n",
    "    \n",
    "    # Convert optical flow to RGB visualization\n",
    "    hsv = np.zeros_like(previous_frame)\n",
    "    hsv[..., 1] = 255\n",
    "    \n",
    "    # Calculating magnitude and angle of optical flow\n",
    "    magnitude, angle = cv2.cartToPolar(flow[..., 0], flow[..., 1])\n",
    "    \n",
    "    # Setting hue according to the angle of optical flow\n",
    "    hsv[..., 0] = angle * 180 / np.pi / 2\n",
    "    \n",
    "    # Setting value according to magnitude of optical flow (normalized)\n",
    "    hsv[..., 2] = cv2.normalize(magnitude, None, 0, 255, cv2.NORM_MINMAX)\n",
    "    \n",
    "    # Convert HSV to RGB\n",
    "    rgb_flow = cv2.cvtColor(hsv, cv2.COLOR_HSV2RGB)\n",
    "    \n",
    "    return rgb_flow\n",
    "\n",
    "def apply_background_subtraction(frame):\n",
    "    \"\"\"Apply background subtraction to focus on moving objects.\"\"\"\n",
    "    # Create background subtractor\n",
    "    if not hasattr(apply_background_subtraction, \"bg_subtractor\"):\n",
    "        apply_background_subtraction.bg_subtractor = cv2.createBackgroundSubtractorMOG2(\n",
    "            history=500, varThreshold=16, detectShadows=True)\n",
    "    \n",
    "    # Apply background subtraction\n",
    "    fg_mask = apply_background_subtraction.bg_subtractor.apply(frame)\n",
    "    \n",
    "    # Clean up the mask (optional)\n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (3, 3))\n",
    "    fg_mask = cv2.morphologyEx(fg_mask, cv2.MORPH_OPEN, kernel)\n",
    "    \n",
    "    # Apply the mask to the original frame\n",
    "    fg_frame = cv2.bitwise_and(frame, frame, mask=fg_mask)\n",
    "    \n",
    "    return fg_frame\n",
    "\n",
    "def augment_frame(frame):\n",
    "    \"\"\"Apply data augmentation to a single frame.\"\"\"\n",
    "    # Define augmentation pipeline\n",
    "    augmenter = A.Compose([\n",
    "        A.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2, p=0.5),\n",
    "        A.GaussianBlur(blur_limit=(3, 7), p=0.3),\n",
    "        A.HorizontalFlip(p=0.5),\n",
    "        A.Rotate(limit=15, p=0.5),\n",
    "        A.RandomGamma(gamma_limit=(80, 120), p=0.3)\n",
    "    ])\n",
    "    \n",
    "    # Apply augmentations\n",
    "    augmented = augmenter(image=frame)\n",
    "    return augmented['image']\n",
    "\n",
    "def convert_color_space(frame, target_space='HSV'):\n",
    "    \"\"\"Convert frame to different color space.\"\"\"\n",
    "    if target_space == 'HSV':\n",
    "        return cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)\n",
    "    elif target_space == 'LAB':\n",
    "        return cv2.cvtColor(frame, cv2.COLOR_BGR2LAB)\n",
    "    elif target_space == 'YCrCb':\n",
    "        return cv2.cvtColor(frame, cv2.COLOR_BGR2YCrCb)\n",
    "    elif target_space == 'GRAY':\n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        return cv2.cvtColor(gray, cv2.COLOR_GRAY2BGR)  # Convert back to 3 channels\n",
    "    else:\n",
    "        return frame  # Default to original frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Frame Extraction and Dataset Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def frames_extraction(video_path, augment=False, compute_flow=False, \n",
    "                      apply_bg_subtraction=False, color_space=None):\n",
    "    \"\"\"Extract and preprocess frames from video with enhanced preprocessing.\"\"\"\n",
    "    frames_list = []\n",
    "    flow_frames = []\n",
    "    \n",
    "    video_reader = cv2.VideoCapture(video_path)\n",
    "    video_frames_count = int(video_reader.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    \n",
    "    # Implement temporal sampling with overlap for smoother sequence\n",
    "    skip_frames_window = max(int(video_frames_count/SEQUENCE_LENGTH), 1)\n",
    "    \n",
    "    # Extract frames\n",
    "    prev_frame = None\n",
    "    \n",
    "    for frame_counter in range(SEQUENCE_LENGTH):\n",
    "        # Set frame position\n",
    "        position = frame_counter * skip_frames_window\n",
    "        video_reader.set(cv2.CAP_PROP_POS_FRAMES, position)\n",
    "        success, frame = video_reader.read() \n",
    "        \n",
    "        if not success:\n",
    "            break\n",
    "            \n",
    "        # Basic preprocessing\n",
    "        resized_frame = cv2.resize(frame, (IMAGE_HEIGHT, IMAGE_WIDTH))\n",
    "        \n",
    "        # Apply background subtraction if requested\n",
    "        if apply_bg_subtraction:\n",
    "            resized_frame = apply_background_subtraction(resized_frame)\n",
    "        \n",
    "        # Convert color space if requested\n",
    "        if color_space:\n",
    "            resized_frame = convert_color_space(resized_frame, color_space)\n",
    "        \n",
    "        # Apply data augmentation if requested\n",
    "        if augment:\n",
    "            resized_frame = augment_frame(resized_frame)\n",
    "        \n",
    "        # Normalize frame\n",
    "        normalized_frame = resized_frame / 255.0\n",
    "        frames_list.append(normalized_frame)\n",
    "        \n",
    "        # Compute optical flow if requested\n",
    "        if compute_flow and prev_frame is not None:\n",
    "            flow = compute_optical_flow(prev_frame, resized_frame)\n",
    "            normalized_flow = flow / 255.0\n",
    "            flow_frames.append(normalized_flow)\n",
    "        \n",
    "        prev_frame = resized_frame.copy()\n",
    "    \n",
    "    video_reader.release()\n",
    "    \n",
    "    # If we're computing flow, make sure we have the right number of frames\n",
    "    if compute_flow:\n",
    "        # We'll have one less flow frame than original frames\n",
    "        if len(flow_frames) < SEQUENCE_LENGTH - 1:\n",
    "            # Duplicate the last flow frame if we're short\n",
    "            flow_frames.append(flow_frames[-1] if flow_frames else np.zeros_like(frames_list[0]))\n",
    "            \n",
    "        # Pad the flow frames to match SEQUENCE_LENGTH if needed\n",
    "        while len(flow_frames) < SEQUENCE_LENGTH:\n",
    "            flow_frames.append(flow_frames[-1] if flow_frames else np.zeros_like(frames_list[0]))\n",
    "        \n",
    "        return frames_list, flow_frames\n",
    "    \n",
    "    return frames_list\n",
    "\n",
    "def create_dataset(augment=False, compute_flow=False, \n",
    "                  apply_bg_subtraction=False, color_space=None,\n",
    "                  max_videos_per_class=None):\n",
    "    \"\"\"Create dataset from video files with enhanced preprocessing options.\"\"\"\n",
    "    features = []\n",
    "    flow_features = []\n",
    "    labels = []\n",
    "    video_files_paths = []\n",
    "    \n",
    "    for class_index, class_name in enumerate(CLASSES_LIST):\n",
    "        print(f'Extracting Data of Class: {class_name}')\n",
    "        files_list = os.listdir(os.path.join(DATASET_PATH, class_name))\n",
    "        \n",
    "        # Limit the number of videos per class if specified\n",
    "        if max_videos_per_class:\n",
    "            files_list = files_list[:max_videos_per_class]\n",
    "            \n",
    "        for file_name in files_list:\n",
    "            video_path = os.path.join(DATASET_PATH, class_name, file_name)\n",
    "            \n",
    "            if compute_flow:\n",
    "                frames, flow = frames_extraction(\n",
    "                    video_path, augment, compute_flow, apply_bg_subtraction, color_space)\n",
    "                \n",
    "                if len(frames) == SEQUENCE_LENGTH and len(flow) == SEQUENCE_LENGTH:\n",
    "                    features.append(frames)\n",
    "                    flow_features.append(flow)\n",
    "                    labels.append(class_index)\n",
    "                    video_files_paths.append(video_path)\n",
    "            else:\n",
    "                frames = frames_extraction(\n",
    "                    video_path, augment, compute_flow, apply_bg_subtraction, color_space)\n",
    "                \n",
    "                if len(frames) == SEQUENCE_LENGTH:\n",
    "                    features.append(frames)\n",
    "                    labels.append(class_index)\n",
    "                    video_files_paths.append(video_path)\n",
    "    \n",
    "    print(f\"Total videos processed: {len(labels)}\")\n",
    "    print(f\"Videos per class: {np.bincount(labels)}\")\n",
    "    \n",
    "    features = np.asarray(features)\n",
    "    labels = np.array(labels)\n",
    "    \n",
    "    if compute_flow:\n",
    "        flow_features = np.asarray(flow_features)\n",
    "        return features, flow_features, labels, video_files_paths\n",
    "    \n",
    "    return features, labels, video_files_paths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Visualization Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_metric(model_training_history, metric_name_1, metric_name_2, plot_name):\n",
    "    \"\"\"Plot training metrics.\"\"\"\n",
    "    metric_value_1 = model_training_history.history[metric_name_1]\n",
    "    metric_value_2 = model_training_history.history[metric_name_2]\n",
    "    epochs = range(len(metric_value_1))\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(epochs, metric_value_1, 'blue', marker='o', label=metric_name_1)\n",
    "    plt.plot(epochs, metric_value_2, 'red', marker='s', label=metric_name_2)\n",
    "    plt.title(str(plot_name))\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Value')\n",
    "    plt.legend()\n",
    "    plt.grid(True, linestyle='--', alpha=0.7)\n",
    "    plt.savefig(f\"{plot_name.replace(' ', '_')}.png\")\n",
    "    plt.show()\n",
    "\n",
    "def visualize_random_frames(features, labels, flow_features=None, num_samples=3):\n",
    "    \"\"\"Visualize random video frames from the dataset.\"\"\"\n",
    "    # Select random indices\n",
    "    if len(features) < num_samples:\n",
    "        num_samples = len(features)\n",
    "        \n",
    "    random_indices = random.sample(range(len(features)), num_samples)\n",
    "    \n",
    "    for i, idx in enumerate(random_indices):\n",
    "        # Get the video and its label\n",
    "        video_frames = features[idx]\n",
    "        video_label = CLASSES_LIST[labels[idx]]\n",
    "        \n",
    "        # Create figure with subplots\n",
    "        fig, axes = plt.subplots(2, 5, figsize=(15, 6))\n",
    "        fig.suptitle(f\"Sample {i+1}: {video_label}\", fontsize=16)\n",
    "        \n",
    "        # Select frames to display (first, middle, and last)\n",
    "        frame_indices = [0, 5, 10, 15, 19]  # Assuming SEQUENCE_LENGTH = 20\n",
    "        \n",
    "        # Plot RGB frames\n",
    "        for j, frame_idx in enumerate(frame_indices):\n",
    "            axes[0, j].imshow(video_frames[frame_idx])\n",
    "            axes[0, j].set_title(f\"Frame {frame_idx}\")\n",
    "            axes[0, j].axis('off')\n",
    "        \n",
    "        # Plot optical flow frames if available\n",
    "        if flow_features is not None:\n",
    "            flow_frames = flow_features[idx]\n",
    "            for j, frame_idx in enumerate(frame_indices):\n",
    "                axes[1, j].imshow(flow_frames[frame_idx])\n",
    "                axes[1, j].set_title(f\"Flow {frame_idx}\")\n",
    "                axes[1, j].axis('off')\n",
    "        else:\n",
    "            for j in range(5):\n",
    "                axes[1, j].axis('off')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.subplots_adjust(top=0.9)\n",
    "        plt.show()\n",
    "        \n",
    "def plot_confusion_matrix(y_true, y_pred, classes):\n",
    "    \"\"\"Plot confusion matrix.\"\"\"\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    import seaborn as sns\n",
    "    \n",
    "    # Convert one-hot encoded labels back to class indices\n",
    "    if len(y_true.shape) > 1 and y_true.shape[1] > 1:  # Check if one-hot encoded\n",
    "        y_true = np.argmax(y_true, axis=1)\n",
    "    \n",
    "    if len(y_pred.shape) > 1 and y_pred.shape[1] > 1:  # Check if one-hot encoded\n",
    "        y_pred = np.argmax(y_pred, axis=1)\n",
    "    \n",
    "    # Compute confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    \n",
    "    # Plot\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=classes, yticklabels=classes)\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.yticks(rotation=0)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"confusion_matrix.png\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model Architectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_two_stream_model():\n",
    "    \"\"\"Create a two-stream ConvLSTM model that processes both RGB and optical flow.\"\"\"\n",
    "    # RGB stream\n",
    "    rgb_input = Input(shape=(SEQUENCE_LENGTH, IMAGE_HEIGHT, IMAGE_WIDTH, 3))\n",
    "    \n",
    "    x1 = ConvLSTM2D(filters=16, kernel_size=(3, 3), activation='tanh',\n",
    "                   data_format=\"channels_last\", recurrent_dropout=0.2,\n",
    "                   return_sequences=True)(rgb_input)\n",
    "    \n",
    "    x1 = BatchNormalization()(x1)\n",
    "    x1 = MaxPooling3D(pool_size=(1, 2, 2), padding='same', data_format='channels_last')(x1)\n",
    "    x1 = TimeDistributed(Dropout(0.25))(x1)\n",
    "    \n",
    "    x1 = ConvLSTM2D(filters=32, kernel_size=(3, 3), activation='tanh',\n",
    "                   data_format=\"channels_last\", recurrent_dropout=0.2,\n",
    "                   return_sequences=True)(x1)\n",
    "    \n",
    "    x1 = BatchNormalization()(x1)\n",
    "    x1 = MaxPooling3D(pool_size=(1, 2, 2), padding='same', data_format='channels_last')(x1)\n",
    "    x1 = TimeDistributed(Dropout(0.25))(x1)\n",
    "    \n",
    "    x1 = Flatten()(x1)\n",
    "    \n",
    "    # Flow stream\n",
    "    flow_input = Input(shape=(SEQUENCE_LENGTH, IMAGE_HEIGHT, IMAGE_WIDTH, 3))\n",
    "    \n",
    "    x2 = ConvLSTM2D(filters=16, kernel_size=(3, 3), activation='tanh',\n",
    "                   data_format=\"channels_last\", recurrent_dropout=0.2,\n",
    "                   return_sequences=True)(flow_input)\n",
    "    \n",
    "    x2 = BatchNormalization()(x2)\n",
    "    x2 = MaxPooling3D(pool_size=(1, 2, 2), padding='same', data_format='channels_last')(x2)\n",
    "    x2 = TimeDistributed(Dropout(0.25))(x2)\n",
    "    \n",
    "    x2 = ConvLSTM2D(filters=32, kernel_size=(3, 3), activation='tanh',\n",
    "                   data_format=\"channels_last\", recurrent_dropout=0.2,\n",
    "                   return_sequences=True)(x2)\n",
    "    \n",
    "    x2 = BatchNormalization()(x2)\n",
    "    x2 = MaxPooling3D(pool_size=(1, 2, 2), padding='same', data_format='channels_last')(x2)\n",
    "    x2 = TimeDistributed(Dropout(0.25))(x2)\n",
    "    \n",
    "    x2 = Flatten()(x2)\n",
    "    \n",
    "    # Fusion\n",
    "    fusion = Concatenate()([x1, x2])\n",
    "    fusion = Dense(256, activation='relu')(fusion)\n",
    "    fusion = Dropout(0.5)(fusion)\n",
    "    \n",
    "    # Output\n",
    "    output = Dense(len(CLASSES_LIST), activation=\"softmax\")(fusion)\n",
    "    \n",
    "    model = tf.keras.Model(inputs=[rgb_input, flow_input], outputs=output)\n",
    "    \n",
    "    # Print model summary\n",
    "    model.summary()\n",
    "    \n",
    "    return model\n",
    "    \n",
    "def create_enhanced_convlstm_model():\n",
    "    \"\"\"Create an enhanced ConvLSTM model architecture.\"\"\"\n",
    "    model = Sequential()\n",
    "    \n",
    "    # First ConvLSTM layer\n",
    "    model.add(ConvLSTM2D(filters=32, kernel_size=(3, 3), activation='relu',\n",
    "                         data_format=\"channels_last\", recurrent_dropout=0.2,\n",
    "                         return_sequences=True, input_shape=(SEQUENCE_LENGTH,\n",
    "                         IMAGE_HEIGHT, IMAGE_WIDTH, 3)))\n",
    "    \n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling3D(pool_size=(1, 2, 2), padding='same', data_format='channels_last'))\n",
    "    model.add(TimeDistributed(Dropout(0.25)))\n",
    "    \n",
    "    # Second ConvLSTM layer with more filters\n",
    "    model.add(ConvLSTM2D(filters=64, kernel_size=(3, 3), activation='relu',\n",
    "                         data_format=\"channels_last\", recurrent_dropout=0.2,\n",
    "                         return_sequences=True))\n",
    "    \n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling3D(pool_size=(1, 2, 2), padding='same', data_format='channels_last'))\n",
    "    model.add(TimeDistributed(Dropout(0.25)))\n",
    "    \n",
    "    # Third ConvLSTM layer\n",
    "    model.add(ConvLSTM2D(filters=128, kernel_size=(3, 3), activation='relu',\n",
    "                         data_format=\"channels_last\", recurrent_dropout=0.2,\n",
    "                         return_sequences=False))  # Last layer doesn't return sequences\n",
    "    \n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.5))\n",
    "    \n",
    "    # Dense layers\n",
    "    model.add(Dense(256, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(len(CLASSES_LIST), activation=\"softmax\"))\n",
    "    \n",
    "    # Print model summary\n",
    "    model.summary()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Data Processing and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [